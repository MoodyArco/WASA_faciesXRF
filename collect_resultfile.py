"""
Created on Mon Mar 18 10:08:35 2019

@author: An-Sheng

This script aims to collect all result.txt files generated by Itrax core scanner and build a single dataset.
Mwanwhile, there are many naming inconsistencies, so I also output some errors to record them.
"""

################## Import Packages #################### 
import time
import os
import shutil
import glob
import pandas as pd
import numpy as np

###### Input all result.txt iteratively ######
result_dir_list = glob.glob('..\\data_original\\*\\**\\*.xrf\\result.txt', recursive = True )

# check if there is any duplicate dirs
if len(pd.Series(result_dir_list).unique()) == len(result_dir_list):
    print('no duplicate dirs')
else: print('some duplicate dirs...need to figure out')

lazy_scan = [] 

# list these problematic cores (two sections scanned in the same time, fragemental scanned)
for section in glob.glob('..\\data_original\\*\\*\\*,*bit.tif'):
    lazy_scan.append(section.split('\\')[3])

# W3-2 section has 4 subsection, so I remove it now and later append those subsections.    
lazy_scan.remove('W3-2')

# core N24 is excluded due to its fragmental scanning, and VVC07 is due to the lack of core description. 
for i in ['N24-1', 'N24-4', 'N24-5', 'VVC07-1', 'VVC07-2', 'VVC07-3', 'VVC07-4', 'W3-2A', 'W3-2B', 'W3-2C', 'W3-2D']:
    lazy_scan.append(i)


result_mega_df = pd.DataFrame()
problems = []
server_dir = []
composite_depth_mm = []
new_spe_dir = []
core_IDS = []
core_sectionS = []
result_count = []

for txt in result_dir_list:
    
    # cathch the core_ID & core_section 
    core_section = txt.split('\\')[-2][:-4].upper()
    
    # deal with the inconsistency of the naming....
    if 'VVC' in core_section:
        core_ID = core_section[:5]
    elif len(core_section) == 4:
        core_ID = core_section[:2]
    elif len(core_section) == 6:
        core_ID = core_section[:4]
    else:
        core_ID = core_section[:3]
    
    if core_section not in lazy_scan:     # only calculate the sections without problem 
        try:
            result_df = pd.read_table(txt, skiprows = 2).drop('Unnamed: 54', axis = 'columns')
        except KeyError:
            problems.append([core_section, 'the setting is different--> inconsistent columns'])
        else:
            for row in range(len(result_df)):
                f = result_df.loc[row, 'filename'].split('\\')
                ser_dir = path[: -14] + txt[3: -10] + 'XRF data\\' + f[-1]
                server_dir.append(ser_dir)
                
                # deal with the inconsistency of the naming....
                if '(' in core_section:
                    col = lith_composite_df.columns[ int(core_section[-4]) ]
                elif ('-' in core_section) & ('_' in core_section):
                    col = lith_composite_df.columns[ int(core_section[-3]) ]
                else:
                    col = lith_composite_df.columns[ int(core_section[-1]) ]
                
                comp_depth = int(
                        result_df.loc[row, 'position (mm)'] + lith_composite_df.loc[core_ID, col]
                        )
                composite_depth_mm.append(comp_depth)
                
                new_dir = (
                        path[: -14] + 'data_composite\\' + core_ID + '\\XRF data'
                        )
                new_spe = (
                        new_dir + '\\{}_{:0>6}.spe'.format(core_ID, comp_depth)
                        )
                new_spe_dir.append(new_spe)
                
                core_IDS.append(core_ID)
                core_sectionS.append(core_section)
                
                # copy the spe to desired data_composite folder
                try:
                    if os.path.isfile(new_spe) == False:    # some spe has been copied at previous runs (1. druing debug 2. overlape scan)
                        if os.path.exists(new_dir):   # if the directory isn't create, create one
                            shutil.copy(ser_dir, new_spe)
                        else:
                            os.makedirs(new_dir, 0o775)
                            shutil.copy(ser_dir, new_spe)
                except FileNotFoundError:
                    problems.append([core_section, 'the original_data has some problems, so it is not copied'])
    
                del ser_dir, comp_depth, new_spe
            
            result_count.append(core_section)
            result_mega_df = result_mega_df.append(result_df, ignore_index = True)
            print(core_section, end = ',\t')
                
result_mega_df['core_ID'] = core_IDS
result_mega_df['core_section'] = core_sectionS
result_mega_df['composite_depth_mm'] = composite_depth_mm
result_mega_df['data_original_dir'] = server_dir
result_mega_df['new_spe_dir'] = new_spe_dir


date = time.strftime('%Y%m%d', time.localtime())
result_mega_df.to_csv(path + '\\WASA_all_xrf_result_{}.csv'.format(date), index = False)
problems
end = time.time()

dur = (end-start)/60
print('{} result.txt are copied and renamed in composite depth'.format(len(result_count)))
print('Process took: {:.2f} mins.'.format(dur))

if os.path.isfile('problems_{}.txt'.format(date)):
    os.remove('problems_{}.txt'.format(date))
with open('problems_{}.txt'.format(date), 'a') as f:
    for line in problems:
        print(line, file = f)
        
if os.path.isfile('processed_sections_{}.txt'.format(date)):
    os.remove('processed_sections_{}.txt'.format(date))
with open('processed_sections_{}.txt'.format(date), 'a') as f:
    for section in result_count:
        print(section, file = f)

# check if there is negative composite depth
with open('negative_depth_spe_{}.txt'.format(date), 'a') as f:
    for spe in result_mega_df.new_spe_dir[result_mega_df.composite_depth_mm < 0]:
        print(spe.split('\\')[-1], file = f)